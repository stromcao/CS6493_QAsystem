import os
import warnings
import logging
import chardet  # æ–°å¢ç¼–ç æ£€æµ‹åº“
from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA

# 1ï¸âƒ£  å±è”½ä¸å¿…è¦çš„è­¦å‘Šï¼Œæ–¹ä¾¿è°ƒè¯•
warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.ERROR)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# 2ï¸âƒ£  å¯åŠ¨æœ¬åœ° Ollama (Mistral-7B)
llm = Ollama(model="mistral")

# 3ï¸âƒ£  è®°å¿†æœºåˆ¶ï¼ˆå­˜å‚¨å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
memory = ConversationBufferMemory()

# 4ï¸âƒ£  å¯¹è¯æ¨¡å¼ï¼ˆæ™®é€šå¯¹è¯ï¼‰
conversation = ConversationChain(llm=llm, memory=memory)

# 5ï¸âƒ£  æ–‡æ¡£é—®ç­”æ¨¡å¼ï¼ˆä¸Šä¼ æ–‡ä»¶åä½¿ç”¨ï¼‰
vector_db = None

def detect_encoding(file_path):
    """ è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬æ–‡ä»¶ç¼–ç  """
    with open(file_path, 'rb') as f:
        result = chardet.detect(f.read())
    return result['encoding']

def load_document(file_path):
    """ åŠ è½½ & å¤„ç†æ–‡æ¡£ """
    global vector_db
    print(f"ğŸ“„  è¯»å–æ–‡ä»¶ï¼š{file_path}")

    try:
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©Loader
        if file_path.lower().endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.lower().endswith(('.docx', '.doc')):
            loader = Docx2txtLoader(file_path)
        else:
            # æ–‡æœ¬æ–‡ä»¶è‡ªåŠ¨æ£€æµ‹ç¼–ç 
            encoding = detect_encoding(file_path)
            loader = TextLoader(file_path, encoding=encoding)

        documents = loader.load()

        # ä¼˜åŒ–æ–‡æœ¬åˆ†å‰²å‚æ•°
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # å¢å¤§å—å¤§å°ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "â€¦â€¦", "â€¦", "ã€€"]  # ä¸­æ–‡å‹å¥½åˆ†éš”ç¬¦
        )
        docs = text_splitter.split_documents(documents)

        # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
        print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æœ¬å—")
        print(f"ğŸ“ é¦–æ–‡æœ¬å—ç¤ºä¾‹ï¼š{docs[0].page_content[:200]}...")

        # å‘é‡åŒ–å¤„ç†
        embeddings = HuggingFaceEmbeddings()
        vector_db = FAISS.from_documents(docs, embeddings)

        return RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_db.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True
        )

    except Exception as e:
        print(f"âŒ æ–‡ä»¶åŠ è½½å¤±è´¥ï¼š{str(e)}")
        return None

qa_chain = None  # å­˜å‚¨é—®ç­”é“¾

# 6ï¸âƒ£  äº¤äº’å¼å¯¹è¯
while True:
    user_input = input("You: ")

    # é€€å‡ºæ¡ä»¶
    if user_input.lower() in ["exit", "quit"]:
        print("ğŸ‘‹  é€€å‡ºå¯¹è¯")
        break

    # åˆ¤æ–­ç”¨æˆ·æ˜¯å¦æåˆ°â€œä¸Šä¼ æ–‡ä»¶â€
    if any(word in user_input.lower() for word in ["ä¸Šä¼ ", "æ–‡ä»¶", "æ–‡æ¡£"]):
        file_path = input("ğŸ“‚ è¯·è¾“å…¥æœ¬åœ°æ–‡ä»¶è·¯å¾„: ").strip()

        # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(file_path):
            print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
            continue

        try:
            qa_chain = load_document(file_path)
            if qa_chain:
                print("âœ… æ–‡ä»¶å·²åŠ è½½ï¼Œç°åœ¨å¯ä»¥åŸºäºæ–‡æ¡£æé—®äº†ï¼")
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥ï¼š{str(e)}")
        continue

    # å¦‚æœå·²ç»ä¸Šä¼ æ–‡ä»¶ï¼Œåˆ™è¿›å…¥æ–‡æ¡£é—®ç­”æ¨¡å¼
    if qa_chain:
        try:
            result = qa_chain({"query": user_input})
            response = f"{result['result']}\n\nğŸ“š æ¥æºæ–‡æ¡£ï¼š{result['source_documents'][0].metadata['source']}"
        except Exception as e:
            response = f"å›ç­”æ—¶å‡ºé”™ï¼š{str(e)}"
    else:
        # æ™®é€šå¯¹è¯æ¨¡å¼
        response = conversation.predict(input=user_input)

    print("Bot:", response)
