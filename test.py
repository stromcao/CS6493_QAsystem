import os
import time
import warnings
import logging
import chardet
import torch  # æ–°å¢æ˜¾å­˜ç›‘æ§ä¾èµ–
from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA

# 1ï¸âƒ£ åˆå§‹åŒ–é…ç½®
warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.ERROR)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# 2ï¸âƒ£ æ¨¡å‹æ³¨å†Œè¡¨ï¼ˆæ ¹æ®é¡¹ç›®æ–‡æ¡£è¦æ±‚å®ç°åŒæ¨¡å‹å¯¹æ¯”ï¼‰
llm_registry = {
    "mistral": Ollama(
        model="mistral",  # é‡åŒ–ç‰ˆæœ¬ï¼ˆç¬¦åˆSmoothQuantå»ºè®®ï¼‰
        temperature=0.7,
        num_ctx=4096  # é•¿ä¸Šä¸‹æ–‡æ”¯æŒ
    ),
    "qwen": Ollama(
        model="qwen:1.8b",     # é‡åŒ–ç‰ˆT5-base
        temperature=0.5,
        num_ctx=2048   # T5çš„æ ‡å‡†ä¸Šä¸‹æ–‡é•¿åº¦
    )
}
current_model = "mistral"  # é»˜è®¤æ¨¡å‹

# 3ï¸âƒ£ ç³»ç»Ÿç»„ä»¶åˆå§‹åŒ–
memory = ConversationBufferMemory()
vector_db = None
qa_chain = None

def detect_encoding(file_path):
    """ è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬æ–‡ä»¶ç¼–ç  """
    with open(file_path, 'rb') as f:
        result = chardet.detect(f.read())
    return result['encoding']

# 4ï¸âƒ£ æ–‡æ¡£å¤„ç†æ¨¡å—ï¼ˆä¿æŒåŸæœ‰åˆ†å—ç­–ç•¥ä¼˜åŒ–ï¼‰
def load_document(file_path):
    global qa_chain, vector_db
    print(f"ğŸ“„  è¯»å–æ–‡ä»¶ï¼š{file_path}")
    try:
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©Loader
        if file_path.lower().endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.lower().endswith(('.docx', '.doc')):
            loader = Docx2txtLoader(file_path)
        else:
            # æ–‡æœ¬æ–‡ä»¶è‡ªåŠ¨æ£€æµ‹ç¼–ç 
            encoding = detect_encoding(file_path)
            loader = TextLoader(file_path, encoding=encoding)

        documents = loader.load()

        # ä¼˜åŒ–æ–‡æœ¬åˆ†å‰²å‚æ•°
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # å¢å¤§å—å¤§å°ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "â€¦â€¦", "â€¦", "ã€€"]  # ä¸­æ–‡å‹å¥½åˆ†éš”ç¬¦
        )
        docs = text_splitter.split_documents(documents)

        # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
        print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æœ¬å—")
        print(f"ğŸ“ é¦–æ–‡æœ¬å—ç¤ºä¾‹ï¼š{docs[0].page_content[:200]}...")

        # å‘é‡åŒ–å¤„ç†
        embeddings = HuggingFaceEmbeddings()
        vector_db = FAISS.from_documents(docs, embeddings)

        # ä½¿ç”¨å½“å‰æ¿€æ´»æ¨¡å‹ç”Ÿæˆé—®ç­”é“¾
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm_registry[current_model],  # åŠ¨æ€ç»‘å®šå½“å‰æ¨¡å‹
            chain_type="stuff",
            retriever=vector_db.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True
        )
        return qa_chain
    except Exception as e:
        print(f"âŒ æ–‡ä»¶åŠ è½½å¤±è´¥ï¼š{str(e)}")
        return None

# 5ï¸âƒ£ äº¤äº’ç³»ç»Ÿå‡çº§ï¼ˆæ–°å¢æ¨¡å‹æ§åˆ¶åŠŸèƒ½ï¼‰
def print_help():
    print("""\nğŸ”§ ç³»ç»ŸæŒ‡ä»¤æ‰‹å†Œï¼š
    /switch [mistral|qwen]  - åˆ‡æ¢å¤§è¯­è¨€æ¨¡å‹
    /compare [é—®é¢˜]       - å¯¹æ¯”æ¨¡å‹æ€§èƒ½
    /help                - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    upload               - ä¸Šä¼ æ–‡æ¡£è¿›å…¥é—®ç­”æ¨¡å¼
    exit/quit            - é€€å‡ºç³»ç»Ÿ""")

print_help()

while True:
    user_input = input("\nYou: ").strip()

    # æŒ‡ä»¤å¤„ç†æ¨¡å—
    if user_input.startswith("/"):
        # æ¨¡å‹åˆ‡æ¢æŒ‡ä»¤
        if user_input.startswith("/switch"):
            model_name = user_input.split()[-1].lower()
            if model_name in llm_registry:
                current_model = model_name
                # æ˜¾å­˜æ¸…ç†ï¼ˆé‡è¦ï¼ï¼‰
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                print(f"ğŸ”„ å·²åˆ‡æ¢è‡³ {model_name.upper()} æ¨¡å‹ï¼ˆæ˜¾å­˜å ç”¨ï¼š{torch.cuda.memory_allocated()/1024**2:.1f}MBï¼‰")
            else:
                print(f"âš ï¸ å¯ç”¨æ¨¡å‹ï¼š{list(llm_registry.keys())}")
            continue

        # æ€§èƒ½å¯¹æ¯”æŒ‡ä»¤ï¼ˆç¬¦åˆé¡¹ç›®æ–‡æ¡£çš„è¯„ä¼°è¦æ±‚ï¼‰
        elif user_input.startswith("/compare"):
            query = user_input[8:].strip()
            if not query:
                print("âŒ è¯·è¾“å…¥å¯¹æ¯”é—®é¢˜ï¼Œä¾‹å¦‚ï¼š/compare è§£é‡Šæ³¨æ„åŠ›æœºåˆ¶")
                continue

            print(f"\nğŸ” æ­£åœ¨å¯¹æ¯”æ¨¡å‹æ€§èƒ½...")
            results = {}
            for name, model in llm_registry.items():
                start_time = time.time()
                try:
                    response = model.invoke(query)
                    latency = time.time() - start_time
                    mem_usage = torch.cuda.memory_allocated()/1024**2 if torch.cuda.is_available() else 0

                    results[name] = {
                        "response": response,
                        "latency": f"{latency:.2f}s",
                        "memory": f"{mem_usage:.1f}MB"
                    }
                except Exception as e:
                    print(f"âŒ {name.upper()} æ¨¡å‹å“åº”å¤±è´¥ï¼š{str(e)}")

            print("\nğŸ†š æ€§èƒ½å¯¹æ¯”ç»“æœï¼š")
            for model, data in results.items():
                print(f"{model.upper()}:")
                print(f"â±ï¸ å“åº”æ—¶é—´: {data['latency']}")
                print(f"ğŸ’¾ æ˜¾å­˜å ç”¨: {data['memory']}")
                print(f"ğŸ“ å“åº”ç¤ºä¾‹: {data['response'][:150]}...\n")
            continue

        elif user_input == "/help":
            print_help()
            continue

    if user_input.lower() in ["exit", "quit"]:
        print("ğŸ‘‹  é€€å‡ºå¯¹è¯")
        break

    # åˆ¤æ–­ç”¨æˆ·æ˜¯å¦æåˆ°â€œä¸Šä¼ æ–‡ä»¶â€
    if any(word in user_input.lower() for word in ["ä¸Šä¼ ", "æ–‡ä»¶", "æ–‡æ¡£"]):
        file_path = input("ğŸ“‚ è¯·è¾“å…¥æœ¬åœ°æ–‡ä»¶è·¯å¾„: ").strip()

        # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(file_path):
            print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
            continue

        try:
            qa_chain = load_document(file_path)
            if qa_chain:
                print("âœ… æ–‡ä»¶å·²åŠ è½½ï¼Œç°åœ¨å¯ä»¥åŸºäºæ–‡æ¡£æé—®äº†ï¼")
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥ï¼š{str(e)}")
        continue


    # 6ï¸âƒ£ åŠ¨æ€ç»‘å®šå½“å‰æ¨¡å‹ï¼ˆå…³é”®æ›´æ–°ç‚¹ï¼ï¼‰
    conversation = ConversationChain(
        llm=llm_registry[current_model],
        memory=memory
    )

    # å¦‚æœå·²ç»ä¸Šä¼ æ–‡ä»¶ï¼Œåˆ™è¿›å…¥æ–‡æ¡£é—®ç­”æ¨¡å¼
    if qa_chain:
        try:
            result = qa_chain({"query": user_input})
            response = f"{result['result']}\n\nğŸ“š æ¥æºæ–‡æ¡£ï¼š{result['source_documents'][0].metadata['source']}"
        except Exception as e:
            response = f"å›ç­”æ—¶å‡ºé”™ï¼š{str(e)}"
    else:
        # æ™®é€šå¯¹è¯æ¨¡å¼
        response = conversation.predict(input=user_input)

    print("Bot:", response)